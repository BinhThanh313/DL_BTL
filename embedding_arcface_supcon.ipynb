{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a6609b",
   "metadata": {},
   "source": [
    "# Học nhúng ảnh đáy mắt với ArcFace và SupCon\n",
    "Notebook này hướng dẫn xây dựng pipeline embedding cho ảnh đáy mắt bằng ArcFace Loss và Supervised Contrastive Loss (SupCon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8936274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\binhd\\appdata\\roaming\\python\\python312\\site-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics) (2.5.1+cu121)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchmetrics) (0.14.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (80.9.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2025.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\binhd\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Cài đặt thư viện nếu cần\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b858525f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1552b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset cho embedding supervised\n",
    "class DRDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 0] + '.png'\n",
    "        label = int(self.data.iloc[idx, 1])\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1f5238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "train_csv = 'aptos2019/train_split.csv'\n",
    "img_dir = 'aptos2019/train_images'\n",
    "train_dataset = DRDataset(train_csv, img_dir, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2552228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mô hình embedding\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, num_classes=5):\n",
    "        super().__init__()\n",
    "        self.backbone = efficientnet_b3(weights=None)\n",
    "        self.backbone.classifier[1] = nn.Linear(self.backbone.classifier[1].in_features, embedding_dim)\n",
    "        self.arcface = ArcFace(embedding_dim, num_classes)\n",
    "    def forward(self, x, labels=None):\n",
    "        emb = self.backbone(x)\n",
    "        if labels is not None:\n",
    "            logits = self.arcface(emb, labels)\n",
    "            return emb, logits\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc51ba",
   "metadata": {},
   "source": [
    "## Sử dụng EfficientNet pretrained và fine-tune cho embedding\n",
    "Bạn có thể sử dụng EfficientNet với trọng số pretrained (ImageNet) để cải thiện chất lượng embedding. Sau đó fine-tune trên dữ liệu của bạn.\n",
    "- Sử dụng weights='IMAGENET1K_V1' khi khởi tạo backbone.\n",
    "- Có thể freeze các layer đầu, chỉ train các layer cuối và lớp embedding.\n",
    "- Áp dụng ArcFace hoặc SupCon như pipeline hiện tại.\n",
    "Ví dụ dưới đây sẽ hướng dẫn cách làm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef021431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArcFace Loss implementation\n",
    "import math\n",
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, s=30.0, m=0.50):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(embedding_dim, num_classes))\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "    def forward(self, emb, labels):\n",
    "        emb_norm = nn.functional.normalize(emb, dim=1)\n",
    "        W_norm = nn.functional.normalize(self.W, dim=0)\n",
    "        logits = torch.matmul(emb_norm, W_norm)\n",
    "        theta = torch.acos(torch.clamp(logits, -1.0, 1.0))\n",
    "        target_logits = torch.cos(theta + self.m)\n",
    "        one_hot = torch.zeros_like(logits)\n",
    "        one_hot.scatter_(1, labels.view(-1,1), 1)\n",
    "        output = logits * (1 - one_hot) + target_logits * one_hot\n",
    "        output = output * self.s\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc0c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SupCon Loss implementation\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    def forward(self, features, labels):\n",
    "        features = nn.functional.normalize(features, dim=1)\n",
    "        batch_size = features.shape[0]\n",
    "        mask = torch.eq(labels.unsqueeze(1), labels.unsqueeze(0)).float()\n",
    "        anchor_dot_contrast = torch.div(torch.matmul(features, features.T), self.temperature)\n",
    "        logits_mask = torch.ones_like(mask) - torch.eye(batch_size, device=mask.device)\n",
    "        mask = mask * logits_mask\n",
    "        exp_logits = torch.exp(anchor_dot_contrast) * logits_mask\n",
    "        log_prob = anchor_dot_contrast - torch.log(exp_logits.sum(1, keepdim=True) + 1e-8)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
    "        loss = -mean_log_prob_pos.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf411063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffbaadde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - ArcFace Loss: 15.9791\n",
      "Epoch 2/50 - ArcFace Loss: 12.8158\n",
      "Epoch 3/50 - ArcFace Loss: 10.7569\n",
      "Epoch 4/50 - ArcFace Loss: 10.1474\n",
      "Epoch 5/50 - ArcFace Loss: 9.4278\n",
      "Epoch 6/50 - ArcFace Loss: 9.1020\n",
      "Epoch 7/50 - ArcFace Loss: 8.3536\n",
      "Epoch 8/50 - ArcFace Loss: 8.2111\n",
      "Epoch 9/50 - ArcFace Loss: 7.8227\n",
      "Epoch 10/50 - ArcFace Loss: 7.0061\n",
      "Epoch 11/50 - ArcFace Loss: 6.9451\n",
      "Epoch 12/50 - ArcFace Loss: 6.6442\n",
      "Epoch 13/50 - ArcFace Loss: 6.3533\n",
      "Epoch 14/50 - ArcFace Loss: 6.0204\n",
      "Epoch 15/50 - ArcFace Loss: 6.1441\n",
      "Epoch 16/50 - ArcFace Loss: 5.6250\n",
      "Epoch 17/50 - ArcFace Loss: 5.2926\n",
      "Epoch 18/50 - ArcFace Loss: 5.2515\n",
      "Epoch 19/50 - ArcFace Loss: 5.0078\n",
      "Epoch 20/50 - ArcFace Loss: 5.1216\n",
      "Epoch 21/50 - ArcFace Loss: 4.8258\n",
      "Epoch 22/50 - ArcFace Loss: 4.7356\n",
      "Epoch 23/50 - ArcFace Loss: 4.6181\n",
      "Epoch 24/50 - ArcFace Loss: 4.9038\n",
      "Epoch 25/50 - ArcFace Loss: 4.6366\n",
      "Epoch 26/50 - ArcFace Loss: 4.9343\n",
      "Epoch 27/50 - ArcFace Loss: 4.1154\n",
      "Epoch 28/50 - ArcFace Loss: 4.7608\n",
      "Epoch 29/50 - ArcFace Loss: 4.2237\n",
      "Epoch 30/50 - ArcFace Loss: 4.0634\n",
      "Epoch 31/50 - ArcFace Loss: 3.9611\n",
      "Epoch 32/50 - ArcFace Loss: 4.5074\n",
      "Epoch 33/50 - ArcFace Loss: 3.8145\n",
      "Epoch 34/50 - ArcFace Loss: 3.7256\n",
      "Epoch 35/50 - ArcFace Loss: 4.1847\n",
      "Epoch 36/50 - ArcFace Loss: 4.0249\n",
      "Epoch 37/50 - ArcFace Loss: 3.7656\n",
      "Epoch 38/50 - ArcFace Loss: 3.7304\n",
      "Epoch 39/50 - ArcFace Loss: 3.7837\n",
      "Epoch 40/50 - ArcFace Loss: 3.5492\n",
      "Epoch 41/50 - ArcFace Loss: 2.9506\n",
      "Epoch 42/50 - ArcFace Loss: 3.7115\n",
      "Epoch 43/50 - ArcFace Loss: 3.2984\n",
      "Epoch 44/50 - ArcFace Loss: 3.0973\n",
      "Epoch 45/50 - ArcFace Loss: 3.5075\n",
      "Epoch 46/50 - ArcFace Loss: 2.9858\n",
      "Epoch 47/50 - ArcFace Loss: 2.8772\n",
      "Epoch 48/50 - ArcFace Loss: 2.8610\n",
      "Epoch 49/50 - ArcFace Loss: 2.8327\n",
      "Epoch 50/50 - ArcFace Loss: 2.6484\n"
     ]
    }
   ],
   "source": [
    "# Huấn luyện embedding với ArcFace\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = EmbeddingNet(embedding_dim=128, num_classes=5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        emb, logits = model(images, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} - ArcFace Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d528ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu lại model ArcFace sau khi train\n",
    "torch.save(model.state_dict(), 'arcface_embedding.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lại model ArcFace khi cần sử dụng\n",
    "model = EmbeddingNet(embedding_dim=128, num_classes=5).to(device)\n",
    "model.load_state_dict(torch.load('arcface_embedding.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d94fa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - SupCon Loss: 4.1446\n",
      "Epoch 2/50 - SupCon Loss: 3.4225\n",
      "Epoch 3/50 - SupCon Loss: 3.4001\n",
      "Epoch 4/50 - SupCon Loss: 3.3687\n",
      "Epoch 5/50 - SupCon Loss: 3.3413\n",
      "Epoch 6/50 - SupCon Loss: 3.3308\n",
      "Epoch 7/50 - SupCon Loss: 3.2836\n",
      "Epoch 8/50 - SupCon Loss: 3.2661\n",
      "Epoch 9/50 - SupCon Loss: 3.3413\n",
      "Epoch 10/50 - SupCon Loss: 3.2022\n",
      "Epoch 11/50 - SupCon Loss: 3.2203\n",
      "Epoch 12/50 - SupCon Loss: 3.1671\n",
      "Epoch 13/50 - SupCon Loss: 3.1306\n",
      "Epoch 14/50 - SupCon Loss: 3.0477\n",
      "Epoch 15/50 - SupCon Loss: 3.0233\n",
      "Epoch 16/50 - SupCon Loss: 2.9469\n",
      "Epoch 17/50 - SupCon Loss: 3.0129\n",
      "Epoch 18/50 - SupCon Loss: 2.9960\n",
      "Epoch 19/50 - SupCon Loss: 2.9518\n",
      "Epoch 20/50 - SupCon Loss: 2.9300\n",
      "Epoch 21/50 - SupCon Loss: 2.9482\n",
      "Epoch 22/50 - SupCon Loss: 2.8584\n",
      "Epoch 23/50 - SupCon Loss: 2.8409\n",
      "Epoch 24/50 - SupCon Loss: 2.7890\n",
      "Epoch 25/50 - SupCon Loss: 2.7965\n",
      "Epoch 26/50 - SupCon Loss: 2.8126\n",
      "Epoch 27/50 - SupCon Loss: 2.8332\n",
      "Epoch 28/50 - SupCon Loss: 2.7654\n",
      "Epoch 29/50 - SupCon Loss: 2.8115\n",
      "Epoch 30/50 - SupCon Loss: 2.7649\n",
      "Epoch 31/50 - SupCon Loss: 2.8358\n",
      "Epoch 32/50 - SupCon Loss: 2.7826\n",
      "Epoch 33/50 - SupCon Loss: 2.7302\n",
      "Epoch 34/50 - SupCon Loss: 2.7407\n",
      "Epoch 35/50 - SupCon Loss: 2.7587\n",
      "Epoch 36/50 - SupCon Loss: 2.7638\n",
      "Epoch 37/50 - SupCon Loss: 2.7143\n",
      "Epoch 38/50 - SupCon Loss: 2.7355\n",
      "Epoch 39/50 - SupCon Loss: 2.6850\n",
      "Epoch 40/50 - SupCon Loss: 2.7408\n",
      "Epoch 41/50 - SupCon Loss: 2.7725\n",
      "Epoch 42/50 - SupCon Loss: 2.7165\n",
      "Epoch 43/50 - SupCon Loss: 2.7300\n",
      "Epoch 44/50 - SupCon Loss: 2.7687\n",
      "Epoch 45/50 - SupCon Loss: 2.7379\n",
      "Epoch 46/50 - SupCon Loss: 2.7356\n",
      "Epoch 47/50 - SupCon Loss: 2.6682\n",
      "Epoch 48/50 - SupCon Loss: 2.6641\n",
      "Epoch 49/50 - SupCon Loss: 2.6939\n",
      "Epoch 50/50 - SupCon Loss: 2.6022\n"
     ]
    }
   ],
   "source": [
    "# Huấn luyện embedding với SupCon\n",
    "model = EmbeddingNet(embedding_dim=128, num_classes=5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = SupConLoss(temperature=0.07)\n",
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        emb = model(images)\n",
    "        loss = criterion(emb, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS} - SupCon Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b8b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu lại model SupCon sau khi train\n",
    "torch.save(model.state_dict(), 'supcon_embedding.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b831dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lại model SupCon khi cần sử dụng\n",
    "model = EmbeddingNet(embedding_dim=128, num_classes=5).to(device)\n",
    "model.load_state_dict(torch.load('supcon_embedding.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
